Political Polarization vs. Fact-checking: Emergence of

impudent lies in social media

Samuel Solgon Santos*

Marcelo de Carvalho Griebeler

Federal University of Rio Grande do Sul

Federal University of Rio Grande do Sul

19/07

Resumo

Diﬁcilmente se poder´a negar que candidatos(as) `a cargos eletivos eventualmente decidem
mentir. O que explica a decis˜ao de pol´ıticos de mentir de forma descarada? Neste tra-
balho mostramos que, uma vez que tenham decidido mentir, os candidatos podem optar
por um n´ıvel de descaramento maior ou menor para a mentira, a depender da polariza¸c˜ao
do eleitorado e do custo de produ¸c˜ao de mentira. Estudamos o problema de um candidato
que deve escolher o n´ıvel de descaramento de uma mentira que ser´a disseminada por i)
eleitores, ii) militantes partid´arios e iii) BOTs em uma rede social. Estes agentes s˜ao
incorporados no modelo de difus˜ao proposto por Bass (1969). Em particular, n´os propo-
mos uma rela¸c˜ao microfundamentada entre as decis˜oes dos eleitores de compartilhamento
(ou n˜ao compartilhamento) de conte´udo na rede social, com o modelo de dissemina¸c˜ao
em rede e, por conseguinte, com os incentivos do candidato a produzir mentiras mais
(ou menos) descaradas. Concretamente, assumimos que o candidato conhece o n´ıvel de
polariza¸c˜ao do eleitorado e o processo de dissemina¸c˜ao e, com base nestas informa¸c˜oes,
escolher´a o grau de descaramento da mentira que maximiza a dissemina¸c˜ao at´e a data da
elei¸c˜ao. Este modelo tem a virtude de relacionar os principais elementos que comumente
s˜ao relacionados `a produ¸c˜ao e dissemina¸c˜ao de mentiras nas redes sociais. O principal
resultado do modelo ´e que o aumento da polariza¸c˜ao do eleitorado gera incentivos para a
produ¸c˜ao de mentiras mais descaradas, e este resultado est´a de acordo com a observa¸c˜ao

*E-mail: samuel.solgon@ufrgs.br. This research was supported in part by Coordination for the Im-

provement of Higher Education Personnel (CAPES).

Address: Universidade Federal do Rio Grande do Sul – UFRGS, Faculdade de Ciˆencias Econˆomicas,
Departamento Economia e Rela¸c˜oes Internacionais, Avenida Jo˜ao Pessoa 52, Centro Hist´orico, Porto Ale-
gre – RS, Brazil, ZIP code: 90040-000 – Phone: 55 (51) 3308-3324. E-mail: marcelo.griebeler@ufrgs.br.

1

de que o recente aumento da polariza¸c˜ao do eleitorado foi acompanhado pela dissemina¸c˜ao
de mentiras descaradas, em particular mentiras sobre temas em pol´ıtica.

Palavras-chave: Redes Sociais. Bass model. Polariza¸c˜ao do Eleitorado. Militantes
Partid´arios. BOTs.

Abstract

What explains the increase in the prevalence of impudent lies among politicians?
Suppose a candidate to an elective oﬃce has decided to lie through a post on a social
media website. After having decided to lie, the candidate must choose the lie’s level of
impudence. We assume the candidate aims to maximize the share of electors who will
make contact with the lie until the election date. We also assume that the spread of the lie
follows an adaptation of the Bass diﬀusion model whereupon we incorporate the actions
of i) electors, ii) candidate’s militants and iii) BOTs. In particular, we incorporate the
elector’s rational behavior and the aggregate level of polarization of the electorate to
the model. In particular, we assume that electors choose rationally whether to share or
not share the candidate’s message. Furthermore, we aggregate these decisions, in such
a way to link elector’s individual decisions to the diﬀusion of the lie, and consequently,
to the candidate’s decision regarding the level of the impudence of the lie. The main
contribution of this model is to link the actors that are often pointed out as the main
factors behind the dissemination of lies in social media websites. Additionally, we show
that the dissemination of the lie depends fundamentally on the polarization level of the
electorate, and that the dissemination is greater when the electorate is more polarized.
Furthermore, we show that the share of electors who accepts to share the lie is greater
when the polarization is higher, and that the candidate chooses accordingly deciding to
send a more impudent lie.

Keywords: Social Media. Bass model. Political Polarization. Political Militants. BOTs.
JEL classiﬁcation: D72, D01, F50
´Area da ANPEC: Microeconomia, M´etodos Quantitativos e Finan¸cas.

1

Introdu¸c˜ao

A produ¸c˜ao e dissemina¸c˜ao de mentiras com objetivos pol´ıtico-partid´arios n˜ao ´e um
fenˆomeno recente, entretanto, as mentiras que foram disseminadas durante as elei¸c˜oes
americanas de 2016 chamaram aten¸c˜ao pelo seu alto grau de descaramento. Not´ıcias como
“Pope Francis Shocks World, Endorses Donald Trump for President” e “ISIS Leader
Calls for American Muslim Voters to Support Hillary Clinton” s˜ao hist´orias inventadas,

2

descaradamente falsas e que foram compartilhadas por diversos usu´arios de redes sociais
durante os meses anteriores `a elei¸c˜ao americana de 2016.

Hist´orias falsas podem ser publicadas por diferentes agentes e com diferentes objetivos.
Adolescentes da Macedˆonia, por exemplo, perceberam que poderiam atrair milhares de
internautas para os seus sites – e assim aumentar sua receita com publicidade – postando
hist´orias inventadas em grupos pol´ıticos do Facebook (Subramanian, 2017). Por outro
lado, a organiza¸c˜ao russa Internet Research Agency foi acusada de produzir e disseminar
mentiras com o objetivo de interferir nos resultados das elei¸c˜oes americanas de 2016
(of America, 2018).

N˜ao obstante, h´a evidˆencias de que candidatos `a presidˆencia tamb´em podem publicar
mentiras descaradas. A agˆencia de checagem de not´ıcias Politifact, por exemplo, classiﬁ-
cou 34% do total das aﬁrma¸c˜oes de Donald Trump analisadas como “falsas” e, 15% (do
total) foram classiﬁcadas como imprecisas e rid´ıculas (Politifact, 2019)1.

Nosso modelo busca iliustrar os incentivos de candidatos `a cargos eletivos (presidˆencia,
por exemplo) a mentir de forma descarada. O modelo que propomos n˜ao endogeniza a
decis˜ao do candidato de mentir, ao contr´ario, parte do pressuposto que o candidato j´a
decidiu mentir e deve escolher o n´ıvel de descaramento da mentira que publicar´a com o
objetivo de maximizar a sua dissemina¸c˜ao at´e a data da elei¸c˜ao.

Assumimos que a dissemina¸c˜ao da mentira nas redes sociais depende da atua¸c˜ao de
trˆes tipos de agentes que denominamos eleitores, militantes e BOTs. N´os assumimos que,
em geral, os eleitores s˜ao bem intencionados e evitam compartilhar mentiras, de forma
que decidem se compartilham a mensagem do candidato com base na probabilidade com
que acreditam que a mensagem ´e verdadeira. Tendo conhecimento do comportamento
“bem–intencionado” dos eleitores, o candidato deve considerar de que forma o grau de
descaramento da mentira ir´a afetar a decis˜ao de compartilhamento dos eleitores.

Note que mentiras descaradas (tais como as citadas no in´ıcio desta Introdu¸c˜ao, por
exemplo) poderiam ser facilmente desmascaradas com uma r´apida pesquisa online, assim,
assumimos que quanto maior o n´ıvel de descaramento da mentira, menor ser´a o seu
custo de checagem e, portanto, mais eleitores estar˜ao dispostos a checa-la. Ainda que
mais eleitores desconﬁem de mentiras descaradas – e, portanto, menos eleitores aceitem
compartilha-las –, mostramos que a polariza¸c˜ao do eleitorado pode induzir o candidato
a escolher mentir mais descaradamente. Ressalta-se que no modelo que propomos, os
eleitores s˜ao penalizados por compartilhar mentiras, de tal forma que parece ser f´acil
concluir que o nosso resultado se manter´a ao incorporarmos a possibilidade de que os
eleitores auﬁram prazer em compartilhar mentiras.

Observa¸c˜oes emp´ıricas exigem que inclua-se BOTs em um modelo de dissemina¸c˜ao
de mentiras em redes sociais. Varol et al. (2017), por exemplo, estimam que entre 9 e

1O site checa novas declara¸c˜oes periodicamente, de forma que as porcentagens apresentadas podem

variar.

3

15% das contas ativas do Twitter s˜ao BOTs, enquanto o Facebook estima que cerca de
60 milh˜oes de BOTs podem estar infectando a plataforma (Watts, 2017).

Al´em disso, consideramos desej´avel considerar a existˆencia de militantes partid´arios
que, diferente dos eleitores, sempre compartilham as mensagens do candidato. A ´unica
especiﬁca¸c˜ao que assumiremos em rela¸c˜ao ao comportamento dos BOTs e militantes
partid´arios ´e que ambos recebem as mensagens diretamente dos candidatos2 e ambos
compartilham estas mensagens, independente do n´ıvel de descaramento. Estas semel-
han¸cas nos levam a tratar BOTs e militantes partid´arios de forma indistinta e, tendo
assim prosseguido, mostramos que a inﬂuˆencia de ambos na dissemina¸c˜ao da mentira ´e
maior para per´ıodos mais pr´oximos `a data da elei¸c˜ao. Adicionalmente, mostramos que,
para um dado n´ıvel de descaramento da mentira, a atua¸c˜ao de BOTs e militantes pode
incentivar eleitores a aceitarem compartilhar a mentira. De forma geral, a atua¸c˜ao de
militantes e BOTs acelera o processo de dissemina¸c˜ao3 e ´e especialmente importante nos
instantes iniciais da propaga¸c˜ao4.

2 Revis˜ao de Literatura

Este trabalho relaciona-se com algumas correntes liter´aria-cient´ıﬁcas que, em geral, n˜ao
se comunicam: a teoria microeconˆomica de ﬁrmas produtoras de not´ıcias, a literatura
de modelos de difus˜ao de inova¸c˜oes e a literatura de modelos de aprendizagem social e
“comportamento de rebanho”.

Parcela importante da literatura sobre ﬁrmas produtoras de not´ıcias buscam explicar
a produ¸c˜ao de not´ıcias viesadas a partir da hip´otese de que consumidores sofrem de vi´es de
sele¸c˜ao e que, assim, buscam consumir not´ıcias de fontes alinhadas `as suas preferˆencias
ideol´ogicas5. Gentzkow and Shapiro (2006), por exemplo, consideram que ﬁrmas po-
dem enviesar consistentemente suas not´ıcias com o objetivo de tornarem-se conhecidas
por certos grupos de indiv´ıduos que buscam consumir informa¸c˜oes adequadas `as suas
predisposi¸c˜oes ideol´ogicas. Ainda, o vi´es de sele¸c˜ao dos indiv´ıduos pode levar a resul-
tados eleitorais ineﬁcientes, tais como a elei¸c˜ao de pol´ıticos mais corruptos (Bernhardt
et al., 2008), ou mesmo inﬂuenciar a escolha de policies pelos governantes (Eisensee and
Str¨omberg, 2007).

H´a diversas ferramentas matem´aticas que s˜ao utilizadas para estudar dissemina¸c˜oes
em rede. Desde a contribui¸c˜ao seminal de Kermack and McKendrick (1991) para a teoria

2Pois, por hip´otese, BOTs e militantes partid´arios seguem a p´agina do candidato na rede social.
3Tal como observado por Lazer et al. (2018); Shao et al. (2018).
4Tal como observado por Shao et al. (2018).
5Al´em dos trabalhos que escolhemos citar no corpo do texto, sugerimos ao leitor interessado na teoria
microeconˆomica por tr´as da produ¸c˜ao de not´ıcias viesadas os trabalhos de Mullainathan and Shleifer
(2005), Suen (2004), Duggan and Martinelli (2011) e o livro organizado por Anderson et al. (2016) que
conta, inclusive, com cap´ıtulos dedicados `as m´ıdias sociais e ao mercado de not´ıcias na internet.

4

matem´atica das epidemias de doen¸cas contagiosas, a literatura de modelos epidˆemicos
passou a ser frequentemente citada como um ferramental natural para estudar a dis-
semina¸c˜ao de informa¸c˜oes em uma sociedade composta por agentes conectados (Jackson,
2010). Ainda, os trabalhos de Daley and Kendall (1964) e Maki and Thompson (1973)
s˜ao referˆencias cl´assicas de modelos que buscam descrever a dissemina¸c˜ao de rumores en-
tre agentes interconectados. Entretanto, foi na literatura de modelos de dissemina¸c˜ao de
inova¸c˜oes entre ﬁrmas que encontramos a abordagem mais adequada aos nossos objetivos.
O modelo de dissemina¸c˜ao que utilizamos foi desenvolvido por Bass (1969) com o
objetivo de prever a dinˆamica de dissemina¸c˜ao de uma inova¸c˜ao. O modelo proposto pelo
autor, juntamente com os trabalhos de Fourt and Woodlock (1960) e Mansﬁeld (1961)
s˜ao os modelos de difus˜ao de inova¸c˜oes mais conhecidos (Mahajan et al., 1990). Diferente
de modelos anteriores que previam o crescimento exponencial do n´umero de usu´arios da
inova¸c˜ao (Fourt and Woodlock, 1960; Haines Jr, 1964), o modelo proposto por Bass prevˆe
que a dinˆamica da difus˜ao da inova¸c˜ao segue um padr˜ao tal como ilustrado pela Figura 4.
De fato, em Bass (1969) s˜ao captadas caracter´ısticas essenciais do processo de imita¸c˜ao,
onde ﬁrmas conectadas por algum la¸co social/econˆomico tomam conhecimento de uma
inova¸c˜ao por meio de outras ﬁrmas que j´a a adotaram. Segundo o autor, ﬁrmas podem
obter conhecimento sobre a inova¸c˜ao consultando uma fonte ex´ogena (tal como um jornal
ou revista especializada) ou pelo contato com alguma outra ﬁrma que j´a tenha adotado
a inova¸c˜ao. A interpreta¸c˜ao para a nossa aplica¸c˜ao ´e an´aloga. Os partidos agem como a
fonte de informa¸c˜ao ex´ogena pela qual alguns usu´arios com vi´es de sele¸c˜ao (militantes)
ter˜ao contato com a not´ıcia. A partir do compartilhamento da publica¸c˜ao do partido por
estes usu´arios iniciais, outros indiv´ıduos passam a ter contato com a mentira – ainda que
inicialmente n˜ao estivessem dispostos a receber informa¸c˜oes do partido. O modelo que
propomos permite avaliar o efeito de diversos fatores sobre a dinˆamica de dissemina¸c˜ao da
mentira do partido, tal como o n´ıvel de descaramento da mentira, o n´ıvel de polariza¸c˜ao
do eleitorado e a for¸ca de atua¸c˜ao de BOTs e militantes.

Por ﬁm, nosso modelo se relaciona com a literatura de aprendizado social. Um prob-
lema importante nesta literatura ´e explicar a emergˆencia de “comportamento de rebanho”
em contextos onde indiv´ıduos racionais tomam decis˜oes sequencialmente observando as
escolhas que foram tomadas anteriormente, tal como no seminal trabalho de Banerjee
(1992). A ideia b´asica por tr´as destes modelos ´e que se os agentes n˜ao tˆem certeza sobre
qual a melhor a¸c˜ao, ent˜ao pode ser racional levar em conta decis˜oes tomadas anteriormente
com base em conjuntos de informa¸c˜ao privada diferentes. Este cen´ario te´orico est´a intima-
mente relacionado `a propaga¸c˜ao de conte´udo em redes sociais onde os usu´arios observam
a quantidade de compartilhamentos anteriores antes de decidir se (re)compartilham um
conte´udo ou n˜ao. Ainda, o fenˆomeno da dissemina¸c˜ao de mentiras nas redes sociais encon-
tra uma explica¸c˜ao te´orica fundamental em Bikhchandani et al. (1992). Em um contexto
de decis˜ao sequencial, os autores mostram que, em algum est´agio do processo, os agentes

5

passam a desprezar a informa¸c˜ao privada que possuem e agem de acordo com as decis˜oes
tomadas anteriormente, gerando o que os autores denominam “Informational Cascades”.
Ainda, o processo de decis˜ao individual que adotamos neste trabalho foi inicialmente
proposto por Papanastasiou (2018) que, assim como o trabalho de Bass (1969), serviu de
ponto de partida para o desenvolvimento do modelo que apresentaremos.

3 Modelo

Nosso modelo b´asico ´e composto por uma massa de eleitores, por um candidato a cargo
eletivo (presidˆencia, por exemplo) e por BOTs e militantes do candidato (ou do partido do
candidato). O candidato divulga uma mentira em uma rede social durante a campanha
para algum cargo eletivo. Tal mensagem pode ser publicada na sua p´agina do Facebook,
ou divulgada como uma not´ıcia ou informa¸c˜ao no Twitter, por exemplo. Os militantes
acompanham as publica¸c˜oes do candidato e, tendo a ela acesso imediato, contribuem com
sua dissemina¸c˜ao entre os eleitores atrav´es do compartilhamento. A fun¸c˜ao dos BOTs no
processo de dissemina¸c˜ao ´e similar `a dos militantes e ser´a discutida com mais detalhes na
se¸c˜ao 3.3. Os eleitores que tiverem contato com a publica¸c˜ao atrav´es do compartilhamento
decidir˜ao entre (re)compartilha-la ou n˜ao com os eleitores que ainda n˜ao tiveram contato.
Observe que, enquanto os militantes obt´em a informa¸c˜ao diretamente da fonte (da p´agina
do partido na rede social), os eleitores a recebem via compartilhamento. Al´em disso, os
militantes sempre as compartilham, enquanto os eleitores decidem se o far˜ao.

Nas se¸c˜oes que seguem descreveremos formalmente os agentes que comp˜oem o modelo
e mostraremos como estes agentes determinam o processo de dissemina¸c˜ao da mentira e
inﬂuenciam o grau de descaramento escolhido pelo candidato.

3.1 Os eleitores

Consideramos uma massa de eleitores que utilizam a rede social (ex.: Facebook) para se
informar. Os eleitores s˜ao indexados por i > I e n˜ao tem certeza quanto a veracidade da
publica¸c˜ao do candidato. Cada eleitor i ´e inteiramente caracterizado pela probabilidade

probabilidade com que o indiv´ıduo i acredita que uma publica¸c˜ao qualquer do candidato

Probi  que deﬁne no espa¸co amostral Mentira M, Verdade V . A medida Probi 
´e deﬁnida ex ante o contato com a publica¸c˜ao, de forma que ProbiV  representa a
seja verdadeira. Denotaremos bi   ProbiV  e interpretaremos esta quantidade como
credibilidade que atribuem ao candidato, e denotaremos a distribui¸c˜ao bi por Fbi e sua
densidade por fbi. Assumimos que ambas sejam diferenci´aveis em todo o dom´ınio e na

a credibilidade que o eleitor i atribui ao candidato. Os eleitores s˜ao heterogˆeneos na

ordem em que se ﬁzer necess´ario.

6

Estamos interessados em modelar o comportamento de eleitores quando estes tˆem
contato com a publica¸c˜ao do candidato por meio do compartilhamento de algum de seus
“amigos” da rede social. Ao ter contato com a publica¸c˜ao, os eleitores decidem em dois
est´agios: no primeiro est´agio decidem se checam o conte´udo da publica¸c˜ao e, no segundo
est´agio, decidem se a compartilham ou n˜ao. Pressupomos que os eleitores s˜ao “bem-
intencionados”, no sentido de que sempre buscam compartilhar informa¸c˜oes verdadeiras
e nunca compartilhar informa¸c˜oes falsas. A ideia ´e que a recompensa psicol´ogica destes
eleitores para cada poss´ıvel a¸c˜ao depende, exclusivamente, da probabilidade com que
acreditam que a a¸c˜ao escolhida ´e ben´eﬁca para a sociedade. Como os eleitores n˜ao tˆem
certeza quanto a veracidade da publica¸c˜ao, eles podem decidir checa-la ou n˜ao no primeiro
est´agio. Assumimos que a a¸c˜ao de checar sempre revela o verdadeiro “estado de mundo”,
qual seja M . Al´em disso, independente da decis˜ao tomada no primeiro est´agio, os eleitores
s˜ao chamados a decidir, no segundo est´agio, se ir˜ao ou n˜ao compartilhar a publica¸c˜ao.

A hip´otese de boa inten¸c˜ao ´e capturada pela seguinte fun¸c˜ao utilidade, que pressupo-

mos ser compartilhada por todos os eleitores

us; V    un; M   1 e us; M   un; V    0

(3.1)

onde “s” representa a a¸c˜ao de compartilhar e “n” representa a a¸c˜ao de n˜ao compartilhar.
Note que a especiﬁca¸c˜ao (3.1) ´e tal que o indiv´ıduos sempre ganha o pay-oﬀ m´aximo
de 1 quando compartilha uma mensagem verdadeira ou quando n˜ao compartilha uma
mensagem mentirosa. Dado que a mensagem do candidato ´e mentirosa por hip´otese, os
eleitores que decidirem checar a publica¸c˜ao certamente ir˜ao decidir n˜ao compartilha-la.
Ainda, como estes eleitores tˆem certeza de que est˜ao agindo em benef´ıcio da sociedade
ir˜ao auferir a recompensa psicol´ogica m´axima. A checagem do conte´udo, entretanto,
implica em um custo de tempo e de esfor¸co investigativo que representaremos por K > R.
Assumimos que o custo de checagem de uma mesma mensagem ´e homogˆeneo entre os
eleitores, de forma que o pay-oﬀ daqueles que decidirem checar a not´ıcia no primeiro
per´ıodo ser´a de 1  K.

Por outro lado, aqueles eleitores que n˜ao checarem a publica¸c˜ao n˜ao ter˜ao certeza
sobre a veracidade do conte´udo e dever˜ao escolher entre compartilhar ou n˜ao com base
na credibilidade que atribuem ao candidato. Segundo a especiﬁca¸c˜ao dada em (1), temos

que uEs   bi e uEn   1  bi, onde uE  denota a utilidade esperada do indiv´ıduo i para

todo i > I.

3.2 A escolha dos eleitores

Dada a simplicidade do espa¸co de escolhas e da fun¸c˜ao utilidade dos eleitores, podemos
deduzir condi¸c˜oes suﬁcientes que os levam a escolher cada uma de suas a¸c˜oes poss´ıveis
sem precisar derivar condi¸c˜oes de primeira ordem.

7

A condi¸c˜ao necess´aria e suﬁciente para que o indiv´ıduo decida checar a not´ıcia ´e que

recompensa por checar a mensagem e agir com certeza em favor da sociedade, 1  K, ser´a

1 K A maxbi, 1 bi6. Agora, note que se o custo de checagem K for maior do que 0, 5, a
sempre menor do que maxbi, 1  bi para todos os eleitores, de tal forma que nenhum
eleitor ir´a checar a mensagem. Por outro lado, se K   0, ent˜ao 1 K A maxbi, 1 bi com
´e igual `a 07. Para evitar estes casos, limitaremos K ao intervalo 0, 0, 5. Isto implica

probabilidade 1, de tal forma que a massa de eleitores que ir˜ao compartilhar a mensagem

que para uma mentira com custo de checagem K, apenas os eleitores mais partid´arios,
com bi C 1  K ir˜ao compartilhar a mentira sem checa-la8.

O eleitor escolher´a n˜ao checar e compartilhar a publica¸c˜ao se, e somente se, bi C
max1K, 1bi9. Ainda, note que se bi C 1bi ent˜ao bi C 0, 5, assim, no que diz respeito a

parcela dos eleitores que ir˜ao compartilhar a publica¸c˜ao do partido (sem checar), podemos
limitar nossa aten¸c˜ao a parcela de indiv´ıduos `a direita de 0, 5 na Figura 1. N˜ao obstante,
vale notar que bi C 0, 5 n˜ao ´e uma condi¸c˜ao suﬁciente para o compartilhamento, haja visto
que, tal como ilustrado pela Figura 2, ´e poss´ıvel que 0, 5 B bi @ 1 K, caso em que o eleitor
decide checar a publica¸c˜ao. Por outro lado, como K > 0, 0, 5, temos 1  K > 0, 5, 1 de
tal forma que se bi A 1  K, ent˜ao automaticamente bi A 1  bi. Vamos pressupor que se
bi   0, 5 o indiv´ıduo escolhe compartilhar a not´ıcia sem veriﬁcar e, assim, conclu´ımos que
bi C 1  K ´e uma condi¸c˜ao suﬁciente para o compartilhamento.

A parcela de indiv´ıduos que, caso recebam a publica¸c˜ao a ir˜ao compartilhar sem

ilustrado pela Figura 2. Para introduzir a no¸c˜ao de polariza¸c˜ao do eleitorado pressupomos

checar ´e dada por qK; a   1  F1  K. Ainda, para qualquer n´ıvel de K > 0, 0, 5,
o tamanho da parcela qK; a depender´a do n´ıvel de polariza¸c˜ao do eleitorado, tal como
a seguinte especiﬁca¸c˜ao para a distribui¸c˜ao de bi no intervalo  0, 1
6 , onde a >  0, 12

Fbi; a  

 bi1 

ab2
i
2

ab3
i
3

(3.2)

a



a partir da qual derivamos a conveniente fun¸c˜ao densidade dada por

fbi, a   ab2

i  abi  1  a~6

(3.3)

e cujos gr´aﬁcos para diferentes valores do parˆametro a s˜ao ilustrados na Figura (1)

A convexidade da curva fbi; a ´e controlada pelo parˆametro a, ao qual doravante nos

6Estamos pressupondo que, em caso de empate, a a¸c˜ao de checar ´e sempre dominada e, portanto, o

eleitor s´o ir´a checar se a desigualdade for estrita.

7Isto ser´a verdade para a fun¸c˜ao de distribui¸c˜ao F   que iremos adotar.
8De fato, h´a evidˆencias de que se uma mentira ´e deﬁnitivamente boa para um partido, ent˜ao a
mesma tende a ser creditada e compartilhada mais facilmente por apoiadores do partido (Garrett and
Weeks, 2013; Shin and Thorson, 2017). Meirick (2013), por exemplo, mostrou que indiv´ıduos que se
identiﬁcaram como “Republicanos” mostraram-se mais suscet´ıveis a uma mentira publicada no Facebook
pela ex-candidata republicana `a vice-presidˆencia Americana, senhora Sarah Palin.

9Estamos assumindo que em caso de empate a a¸c˜ao de “n˜ao checar e compartilhar” domina qualquer

outra a¸c˜ao.

8

Figure 1: Densidades de bi para diferentes valores de a

fbi; a

a   12

a   7

a   2

a   0

0.5

1

bi

Figure 2: Massa de eleitores que compartilham a publica¸c˜ao

fbi; a

a   7

B

a   2

A

1  K

1

bi

referiremos como “n´ıvel de polariza¸c˜ao”. Como ilustrado na Figura (1), a convexidade de

veniˆencias importantes, entretanto alguns ressalvas devem ser feitas. A primeira van-

fbi; a ´e crescente no n´ıvel de polariza¸c˜ao, com fbi; a   0 correspondendo `a distribui¸c˜ao
uniforme e o caso fbi; a   12 representando o n´ıvel de polariza¸c˜ao m´axima do eleitorado.
A estrat´egia de propor uma forma funcional espec´ıﬁca para fbi; a carrega con-
tagem ´e que fbi; a capta a ideia de que um eleitorado mais polarizado ´e composto por
publicada sem veriﬁc´a-la. Esta ideia ´e ilustrada pelo fato de que, para todo K > 0, 0, 5,
a ´area `a direta de 1K e abaixo da curva fbi; a, ´e crescente no parˆametro de polariza¸c˜ao
a10. Al´em disso, o gr´aﬁco da densidade fbi, a que propomos facilita a visualiza¸c˜ao do

mais indiv´ıduos com cren¸cas extremas e que, eventualmente ir˜ao compartilhar a mentira

mecˆanismo pelo qual diferentes valores de K - que ser´a a vari´avel de escolha do can-

10Este fato pode ser veriﬁcado bastando mostrar que a ´area B ´e maior do que a ´area A ilustradas na

Figura (2).

9

didato - implicam em diferentes parcelas qK; a de eleitores que, ao serem expostos `a

publica¸c˜ao, decidem compartilha-la sem veriﬁcar. A ressalva que deve ser levantada ´e
que, dado o nosso atual desconhecimento emp´ırico sobre o formato da distribui¸c˜ao real

de bi, a estrat´egia mais segura seria propor uma fbi, a t˜ao geral quanto poss´ıvel, de tal
forma que qualquer que fosse a distribui¸c˜ao real de bi, a mesma estaria contemplada pela
teoria. De fato, resultados obtidos sob hip´oteses mais fracas s˜ao, na verdade, mais fortes
e, sendo assim, um poss´ıvel desenvolvimento posterior deste trabalho ´e investigar se os
resultados aqui obtidos se mant´em sob especiﬁca¸c˜oes mais gerais para a distribui¸c˜ao de
bi.

3.3 A dissemina¸c˜ao da mentira e os militantes partid´arios

Queremos representar a dissemina¸c˜ao da publica¸c˜ao entre indiv´ıduos conectados em uma
rede. Para tanto iremos considerar apenas determinantes “globais” do processo de dis-
semina¸c˜ao, n˜ao levando em considera¸c˜ao propriedades estruturais / topol´ogicas da rede.
O modelo de dissemina¸c˜ao que adotaremos foi inicialmente proposto por Bass (1969) e

com a publica¸c˜ao do partido at´e t.

´e baseado na dinˆamica intertemporal de Gt   parcela de eleitores que tiveram contato
Segundo a interpreta¸c˜ao de Lekvall and Wahlbin (1973), a parcela Gt varia por

inﬂuˆencia de fatores “internos” e “externos”. A inﬂuˆencia interna representa a atua¸c˜ao
de eleitores que, ao terem contato com a publica¸c˜ao decidem compartilha-la. A inﬂuˆencia
externa, por sua vez, representa a atua¸c˜ao de BOTs e militˆantes partid´arios que recebem
a publica¸c˜ao diretamente do candidato, e recebendo-a, compartilham-na imediatamente
atingindo novos eleitores.

A distin¸c˜ao b´asica entre a inﬂuˆencia interna e externa ´e que a primeira representa a
dissemina¸c˜ao “boca-a-boca”, enquanto a segunda representa a dissemina¸c˜ao via BOTs e
militantes que independem da intermedia¸c˜ao de outros agentes para terem contato com
a publica¸c˜ao.

Formalmente, as inﬂuˆencias internas e externas deﬁnem a varia¸c˜ao de Gt da seguinte

forma:

dGt

dt

  qK; aGt1  Gt  p1  Gt p A 0

(3.4)

no processo de dissemina¸c˜ao e relaciona o processo de dissemina¸c˜ao com o comportamento

O termo qK; aGt1Gt ´e a inﬂuˆencia interna. Representa a atua¸c˜ao de eleitores
dos eleitores. A importˆancia de qK; a no processo de dissemina¸c˜ao ´e imediata: quanto
a taxa de dissemina¸c˜ao dada por dGt~dt. O termo Gt por sua vez, ´e o principal

maior a parcela de eleitores que aceitam compartilhar a mentira sem checa-la maior ser´a

meio pelo qual captamos a dissemina¸c˜ao “boca-a-boca” no modelo. A interpreta¸c˜ao ´e de

10

que quanto maior a parcela de eleitores que j´a tiveram contato com a publica¸c˜ao, mais
indiv´ıduos podem dissemin´a-la e, consequentemente, maior a probabilidade de um novo

contato em t. Por ´ultimo, o termo 1  Gt incorpora o fato de que para que dGt~dt A 0

´e necess´ario que ainda hajam indiv´ıduos que n˜ao tiveram contato com a publica¸c˜ao at´e
t.

O termo p1  Gt, por sua vez, ´e a inﬂuˆencia externa e representa a atua¸c˜ao de

militantes partid´arios e BOTs no processo de dissemina¸c˜ao.A inﬂuˆencia destes agentes
ainda depende da existˆencia de indiv´ıduos que n˜ao tiveram contato com a not´ıcia –

1Gt–, entretanto, a hip´otese ´e que BOTs e militantes partid´arios “seguem” a p´agina do

candidato na rede social e, portanto, independem da intermedia¸c˜ao de outros indiv´ıduos
para terem contato com a publica¸c˜ao11.

A equa¸c˜ao (3.4) juntamente com a hip´otese G0   0 tem solu¸c˜ao bem conhecida dada

por

, onde q   qK; a

1  epqt

1 q~pepqt
Figure 3: Fun¸c˜ao gt  

dGt

dt

(3.5)

Gt  

dGt

dt

p

T 

t

Por (3.4) veriﬁcamos que a condi¸c˜ao inicial G0   0 implica que a taxa dG0~dt

´e exclusivamente determinada pela participa¸c˜ao dos militantes. Este fato ´e ilustrado
pelo intercepto vertical do gr´aﬁco da Figura 3. Na medida em que a publica¸c˜ao se
dissemina, a participa¸c˜ao dos eleitores aumenta e sobrecompensa a queda da participa¸c˜ao
dos militantes, de forma que a taxa de dissemina¸c˜ao acelera at´e atingir seu m´aximo
no per´ıodo T . A partir de ent˜ao, a dissemina¸c˜ao segue `a taxas decrescentes at´e que

limt ª Gt   1, tal como ilustrado pela Figura 412.

11A atua¸c˜ao de BOTS e militantes nos parece igualmente bem modeladas pelo termo de inﬂuˆencia
externa, entretanto, nosso modelo n˜ao ´e soﬁsticado o suﬁciente para distingui-las. No decorrer do texto
interpretaremos o termo de inﬂuˆencia externa como captando a atua¸c˜ao de militantes.

12Vale refor¸car que a previs˜ao do modelo ´e de que todos os eleitores ter˜ao contato com a publica¸c˜ao,

11

Figure 4: Dinˆamica da dissemina¸c˜ao

Gt

1

T 

5

t

Como detalharemos na pr´oxima subse¸c˜ao, estamos particularmente interessados em
estudar a parcela de eleitores que tiveram contato com a publica¸c˜ao at´e a data da elei¸c˜ao.
Para tanto, ressaltamos que a dinˆamica da dissemina¸c˜ao ´e inteiramente determinada pelos
parˆametros p e q, e pressupomos a importante hip´otese de que

Hip´otese 1 Os parˆametros p e q s˜ao tais que a taxa de dissemina¸c˜ao ´e m´axima na data
da vota¸c˜ao.

A Hip´otese 1 ´e motivada pela observa¸c˜ao de que o interesse dos eleitores em rela¸c˜ao aos
partidos e candidatos parece atingir seu pico durante o dia da vota¸c˜ao. Veja por exemplo
os gr´aﬁcos da Figura 513 extra´ıdos do Google Trends que mostram que, na ´ultima elei¸c˜ao
presidencial do Brasil e dos Estado Unidos houveram picos de interesse pelos principais
candidatos durante o dia de vota¸c˜ao14.

A fun¸c˜ao da Hip´otese 1 ´e pressupor que a taxa de dissemina¸c˜ao da publica¸c˜ao do
partido ´e fortemente correlacionada ao interesse dos eleitores pelos candidatos (ilustrado
pela Figura 5), de tal forma que a taxa de dissemina¸c˜ao tamb´em ir´a atingir seu m´aximo
na data da vota¸c˜ao. Os valores de q e p deﬁnem a dinˆamica de dissemina¸c˜ao e, em
particular, determinam quanto tempo levar´a para que a taxa de dissemina¸c˜ao da pub-
lica¸c˜ao, representada pela Figura 3, atinja seu ponto de m´aximo. Estamos assumindo
que o partido emite a publica¸c˜ao no instante t   0, assim, se T  > R ´e o intervalo de

e n˜ao de que a publica¸c˜ao ser´a compartilhada por todos.

13Os eixos verticais dos gr´aﬁcos da Figura 5 representam o interesse de pesquisa relativo ao ponto mais
alto no gr´aﬁco em um dado per´ıodo e em uma regi˜ao espec´ıﬁca (no caso da elei¸c˜ao brasileira, o Brasil e
no caso da elei¸c˜ao americana, os Estados Unidos). Um valor de 100 representa o pico de popularidade
de um termo. Um valor de 50 signiﬁca que o termo teve metade da popularidade. Uma pontua¸c˜ao de 0
signiﬁca que n˜ao havia dados suﬁcientes sobre o termo.

14Ainda que o pico de interesse n˜ao tenha ocorrido exatamente no dia da vota¸c˜ao no caso da elei¸c˜ao
americana, ´e ineg´avel que a data da vota¸c˜ao ´e o fator por tr´as do crescimento abrupto do interesse dos
internautas sobre os candidatos Donald Trump e Hillary Clinton.

12

Figure 5: Google trends

Bolsonaro e Haddad

Bolsonaro
Haddad

o
n
r
u
t

o
1

o
n
r
u
t

o
2

8
1
/
1
1
/
0
1

100

80

60

40

20

o
p
m
e
t

o
d

o
g
n
o
l

o
a

e
s
s
e
r
e
t
n
I

0

8
1
/
9
0
/
4
1

tempo entre a publica¸c˜ao e a data de vota¸c˜ao, podemos simplesmente dizer que a T  ´e a
data da vota¸c˜ao. Logo, nossa hip´otese implica que q e p tˆem que ser tais que a taxa de
dissemina¸c˜ao seja m´axima no per´ıodo t   T .

necess´ario para que, a partir da data emiss˜ao, a publica¸c˜ao atinja sua taxa de dissem-

Igualando a derivada da fun¸c˜ao dG~dt `a zero conclu´ımos que a quantidade de tempo
ina¸c˜ao m´axima ´e dado por lnq~p~p  q. Assim, nossa hip´otese implica que, sendo T 
e q devem ser tais que lnq~p~p  q   T .

o intervalo de tempo entre a publica¸c˜ao do partido e a data da elei¸c˜ao, os parˆametros p

Proposi¸c˜ao 1 Com rela¸c˜ao `a atua¸c˜ao dos militantes, temos:

a) Se a distribui¸c˜ao de bi for constante em rela¸c˜ao ao tempo, ent˜ao a atua¸c˜ao dos
militantes ´e positivamente relacionada com a proximidade das datas de publica¸c˜ao da
mensagem e de vota¸c˜ao.

b) Se qK; a @ 1~T , ent˜ao aumentos da atua¸c˜ao dos militantes implicam no aumento

da atua¸c˜ao dos eleitores.

As proposi¸c˜oes acima derivam da restri¸c˜ao de que a taxa de dissemina¸c˜ao deve ser
m´axima na data da vota¸c˜ao e, al´em disso, caracterizam o comportamento de militantes e
eleitores em fun¸c˜ao da proximidade da data da publica¸c˜ao com a data de vota¸c˜ao. Ambas
pq lnq~p   T  e a validade de cada uma depende
foram obtidas a partir da restri¸c˜ao
das hip´oteses que aceitarmos pressupor sobre a distribui¸c˜ao de bi. Se assumirmos que
Fbi ´e constante, ent˜ao tamb´em o ser´a a parcela qK; a. Neste caso, a Proposi¸c˜ao 2-a)

mostra que a atua¸c˜ao dos militantes ser´a maior para publica¸c˜oes emitidas mais pr´oximas

1

13

100

80

60

40

20

o
p
m
e
t

o
d

o
g
n
o
l

o
a

e
s
s
e
r
e
t
n
I

0

6
1
/
0
1
/
0
2

Trump e Hillary

Trump
Hillary

o
˜a
¸c
a
t
o
v

6
1
/
1
1
/
0
3

`a data da vota¸c˜ao. Por outro lado, se acreditarmos que a parcela qK; a muda em

fun¸c˜ao da proximidade da data de publica¸c˜ao com a data da vota¸c˜ao, ent˜ao a Proposi¸c˜ao
2-b) nos permite caracterizar esta mudan¸ca. De fato, a segunda proposi¸c˜ao aﬁrma que
se a publica¸c˜ao for emitida suﬁcientemente pr´oxima da data da vota¸c˜ao, ent˜ao a parcela
de eleitores que aceita compartilhar a publica¸c˜ao do partido responde positivamente a
aumentos na atua¸c˜ao dos militantes.

3.4 O Candidato
Assumimos que o candidato gostaria de maximizar a parcela GT  de eleitores que
ter˜ao contato com a publica¸c˜ao at´e a data da vota¸c˜ao15. Buscando maximizar GT , o

candidato dever´a levar em conta o n´ıvel de polariza¸c˜ao do eleitorado e o parˆametro de
inﬂuˆencia externa para, ent˜ao, decidir o n´ıvel de descaramento da mentira que publicar´a.
Para incentivar os eleitores a compartilhar sua publica¸c˜ao (desincentivar a checagem),
o candidato tem incentivos a publicar uma mentira com alto custo de inspe¸c˜ao K >

0, 0, 5. O custo de inspe¸c˜ao – em que incorrem eleitores que decidam checar a publica¸c˜ao

– ´e proporcional ao n´ıvel de detalhamento e complexidade da mentira, bem como seu
potencial em se passar por informa¸c˜ao verdadeira. A produ¸c˜ao de uma mentira com estas
caracter´ısticas demanda diversos recursos tais como tempo, capital humano e recursos
ﬁnanceiros e assumimos que a demanda por estes insumos ´e proporcional ao n´ıvel de

15Argumentamos que esta fun¸c˜ao objetivo faz sentido na medida em que os eleitores que checam e
desmascaram a publica¸c˜ao n˜ao compartilham este fato com os demais, assim, o candidato n˜ao precisa
se preocupar com os efeitos adversos de uma eventual contra-corrente de compartilhamentos de que sua
publica¸c˜ao foi mentirosa.

14

complexidade e detalhamento da mentira. Em suma, ´e mais custoso (para o candidato)
produzir mentiras com maior custo de inspe¸c˜ao. Formalmente, para produzir uma mentira

com custo de inspe¸c˜ao K > 0, 0, 5, o candidato incorre no custo cK. Ainda, assumimos

que al´em de ser crescente e cont´ınua, a fun¸c˜ao custo ´e convexa e

H.1 lim

K 0

cK   0 e H.2 lim

K 0,5

cK   ª

(3.6)

A hip´otese H.1 capta o simples fato de que ´e muito barato produzir as primeira
unidades de disfarce da mentira. Ainda, esta hip´otese incentiva o partido a investir
algum montante positivo de recursos na produ¸c˜ao da mentira e, assim, obter alguma
parcela positiva de eleitores que a ir˜ao compartilhar, tal como parece ocorrer na realidade.

A hip´otese H.2, em conjunto com a convexidade de c , implica que mentiras muito

soﬁsticadas tˆem seu custo marginal cada vez maior. Dito de outra forma, tornar uma
mentira mais plaus´ıvel ´e mais dif´ıcil quanto mais plaus´ıvel ela j´a ´e. Se imaginarmos que
temos uma mentira muito descarada (K pequeno), torn´a-la um pouco mais plaus´ıvel
´e mais simples – e menos custoso – do que fazer o mesmo para uma mentira menos
descarada (com K alto), que j´a se parece muito com uma aﬁrma¸c˜ao verdadeira.

Assumimos que o partido conhece a regra de decis˜ao dos eleitores e o n´ıvel de polar-
iza¸c˜ao, de forma que tem pleno conhecimento do efeito de sua escolha sobre a parcela

qK; a e sobre a dinˆamica de dissemina¸c˜ao dada pela equa¸c˜ao (3.5).

3.4.1 An´alise do Problema do Candidato

Formalmente, nossa hip´otese ´e de que o candidato resolve

max

K >0, 0,5

GT , K; a  cK

(3.7)

onde GT , K ; a   1~2 1  p~qK; a e qK; a    2aK 3  3aK 2  Ka  6~6. Antes

de enunciar o resultado principal da se¸c˜ao, notamos que

Lema 1 O problema dado pela equa¸c˜ao (3.7) tem solu¸c˜ao ´unica, a qual denotaremos por
K .

E ent˜ao, estamos prontos para aﬁrmar que

Proposi¸c˜ao 2 Quando o n´ıvel de polariza¸c˜ao do eleitorado aumenta, o candidato publica
mentiras mais descaradas.

A proposi¸c˜ao acima garante que o efeito da polariza¸c˜ao sobre K  ´e mon´otono e vale

para qualquer n´ıvel de a >  0, 12 e dos parˆametros p e q. Isto ocorre porque o aumento

da polariza¸c˜ao o candidato ´e capaz de “ludibriar” a mesma parcela de eleitores utilizando
uma mentira com K menor.

15

De fato, mostramos que o benef´ıcio marginal de aumentar o custo de inspe¸c˜ao, dado

por

3p6aK 2  6aK  a  6
2aK 3  3aK 2  Ka  62

(3.8)

´e decrescente no n´ıvel de polariza¸c˜ao. Assim, na medida em que a polariza¸c˜ao au-
menta, K  diminui para compensar a queda do benef´ıcio marginal. Al´em disso, o custo

marginal cK ´e independente de a, de forma que somente o benef´ıcio marginal ´e afetado

pela varia¸c˜ao na polariza¸c˜ao do eleitorado.

4 Prova das Proposi¸c˜oes

Por conta da limita¸c˜ao do n´umero de p´aginas permitidas para esta publica¸c˜ao, iremos
fornecer provas bastante curtas e, em alguns casos, apenas indica¸c˜oes.
Proposi¸c˜ao 1–a) e 1–b)

Prova: As provas das Proposi¸c˜oes 1–a) e 1–b) seguem diretamente do Teorema da

Fun¸c˜ao Impl´ıcita aplicado `as express˜oes q  p expTep  q   0 e q~eTeq  peTep   0,
respectivamente (ambas derivadas da restri¸c˜ao lnq~p~p  q   T  obtida a partir da

Hip´otese 1).
Lema 1 (Existˆencia e Unicidade de solu¸c˜ao para o problema do partido):

Prova: Vamos deﬁnir a fun¸c˜ao JK, a, p baseada na CPO do problema do partido

JK, a, p  

3p6aK 2  6aK  a  6
2aK 3  3aK 2  Ka  62

 cK   0

(4.1)

Para veriﬁcar a existˆencia de K > 0, 0, 5 que satisfaz (4.1), basta notar que JK, a, p
Para garantir a unicidade da solu¸c˜ao, basta mostrar que, para todo a >  0, 12 a fun¸c˜ao

´e cont´ınua e que limK 0 JK, a, p   ª e limK 0,5 JK, a, p   ª.
JK, a, p decresce monotonamente no seu dom´ınio K > 0, 0.5.

Proposi¸c˜ao 2

Prova: Esta prova ´e baseada no Teorema da Fun¸c˜ao Impl´ıcita. Tomando o diferencial

total da fun¸c˜ao JK, a, p, obtemos

∂K
∂a

  

∂J~∂a
∂J~∂K

O resultado segue da observa¸c˜ao (n˜ao trivial) de que ∂K~∂a @ 0.

(4.2)

References

Anderson, S. P., Waldfogel, J., and Stromberg, D. (2016). Handbook of Media Economics,

vol 1A. Elsevier.

16

Banerjee, A. V. (1992). A simple model of herd behavior. The quarterly journal of

economics, 107(3):797–817.

Bass, F. M. (1969). A new product growth for model consumer durables. Management

science, 15(5):215–227.

Bernhardt, D., Krasa, S., and Polborn, M. (2008). Political polarization and the electoral

eﬀects of media bias. Journal of Public Economics, 92(5-6).

Bikhchandani, S., Hirshleifer, D., and Welch, I. (1992). A theory of fads, fashion, cus-
tom, and cultural change as informational cascades. Journal of political Economy,
100(5):992–1026.

Daley, D. J. and Kendall, D. G. (1964). Epidemics and rumours. Nature, 204(4963):1118.

Duggan, J. and Martinelli, C. (2011). A spatial theory of media slant and voter choice.

The Review of Economic Studies, 78(2):640–666.

Eisensee, T. and Str¨omberg, D. (2007). News droughts, news ﬂoods, and us disaster

relief. The Quarterly Journal of Economics, 122(2):693–728.

Fourt, L. A. and Woodlock, J. W. (1960). Early prediction of market success for new

grocery products. Journal of marketing, 25(2):31–38.

Garrett, R. K. and Weeks, B. E. (2013). The promise and peril of real-time corrections to
political misperceptions. In Proceedings of the 2013 conference on Computer supported
cooperative work, pages 1047–1058. ACM.

Gentzkow, M. and Shapiro, J. M. (2006). Media bias and reputation. Journal of political

Economy, 114(2):280–316.

Haines Jr, G. H. (1964). A theory of market behavior after innovation. Management

Science, 10(4):634–658.

Jackson, M. O. (2010). Social and economic networks. Princeton university press.

Kermack, W. O. and McKendrick, A. G. (1991). Contributions to the mathematical

theory of epidemics-i. Bulletin of mathematical biology, 53(1-2):33–55.

Lazer, D. M., Baum, M. A., Benkler, Y., Berinsky, A. J., Greenhill, K. M., Menczer, F.,
Metzger, M. J., Nyhan, B., Pennycook, G., Rothschild, D., et al. (2018). The science
of fake news. Science, 359(6380):1094–1096.

Lekvall, P. and Wahlbin, C. (1973). A study of some assumptions underlying innovation

diﬀusion functions. The Swedish journal of economics, pages 362–377.

17

Mahajan, V., Muller, E., and Bass, F. M. (1990). New product diﬀusion models in

marketing: A review and directions for research. Journal of marketing, 54(1):1–26.

Maki, D. P. and Thompson, M. (1973). Mathematical models and applications: with

emphasis on the social life, and management sciences. Technical report.

Mansﬁeld, E. (1961). Technical change and the rate of imitation. Econometrica: Journal

of the Econometric Society, pages 741–766.

Meirick, P. C. (2013). Motivated misperception? party, education, partisan news, and
belief in “death panels”. Journalism & Mass Communication Quarterly, 90(1):39–57.

Mullainathan, S. and Shleifer, A. (2005). The market for news. American Economic

Review, 95(4):1031–1053.

of America, U. S. (2018). Disctrict of columbia. https://www.justice.gov/file/

1035477/download. Acessado em: 2019-01-26.

Papanastasiou, Y. (2018). Fake news propagation and detection: A sequential model.

Available at SSRN 3028354.

Politifact (2019). Donald trump’s ﬁle. https://www.politifact.com/personalities/

donald-trump/. Acessado em: 2019-02-10.

Shao, C., Ciampaglia, G. L., Varol, O., Yang, K.-C., Flammini, A., and Menczer, F.
(2018). The spread of low-credibility content by social bots. Nature communications,
9(1):4787.

Shin, J. and Thorson, K. (2017). Partisan selective sharing: The biased diﬀusion of

fact-checking messages on social media. Journal of Communication, 67(2):233–255.

Subramanian, S. (2017). Inside de macedonian fake-news complex. https://www.wired.

com/2017/02/veles-macedonia-fake-news/. Acessado em: 2019-01-07.

Suen, W. (2004). The self-perpetuation of biased beliefs. The Economic Journal,

114(495):377–396.

Varol, O., Ferrara, E., Davis, C. A., Menczer, F., and Flammini, A. (2017). Online human-
bot interactions: Detection, estimation, and characterization. In Eleventh international
AAAI conference on web and social media.

Watts, C. (2017). Extremist content and russian disinformation online: Working with tech
to ﬁnd solutions. Statement prepared for the Senate Judiciary Committee, Subcommittee
on Crime and Terrorism.

18

